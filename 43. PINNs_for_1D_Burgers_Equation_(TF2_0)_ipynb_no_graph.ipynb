{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PINNs for 1D Burgers Equation (TF2.0).ipynb_no_graph",
      "provenance": [],
      "collapsed_sections": [
        "A1Mj-RBCn8MZ",
        "GkimJNtepkKi",
        "QGd5zVtoxAqt",
        "bXtJ5GiaxAqw",
        "dOPzdkKsJzA4",
        "OTxvp1nJGDeb",
        "fGrMDRc3w1ex",
        "rRGW4IW0w1e0"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:light",
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://github.com/pierremtb/PINNs-TF2.0"
      ],
      "metadata": {
        "id": "TKDuovkI0gZn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1Mj-RBCn8MZ"
      },
      "source": [
        "# 0. Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al80f-lPoJjh"
      },
      "source": [
        "## Getting the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3iHOMsdnNiq",
        "outputId": "66e3a7e2-47d8-425d-f001-d0ff48452ff1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/maziarraissi/PINNs"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PINNs' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtoc_dXgoOZq"
      },
      "source": [
        "## Setting up modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNMtDjXkFHaN"
      },
      "source": [
        "TeX packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaZCKcDsEVRP"
      },
      "source": [
        "!sudo apt-get -qq install texlive-fonts-recommended texlive-fonts-extra dvipng"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otc4Ap7qFMlf"
      },
      "source": [
        "Pip modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srpq4aQNoQ1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc80c82-73e3-4425-a0d5-d57935811bc9"
      },
      "source": [
        "!pip install tensorflow-gpu pyDOE"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.7/dist-packages (2.9.1)\n",
            "Requirement already satisfied: pyDOE in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.26.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.9.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (21.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.47.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (14.0.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.7.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-gpu) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksKujMvUFRNW"
      },
      "source": [
        "## Imports, config, and utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEDn2fqlqctT",
        "lines_to_next_cell": 2
      },
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODGREPvZpqUz"
      },
      "source": [
        "burgersutil.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgPvqJiYFnYG",
        "lines_to_next_cell": 0
      },
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pyDOE import lhs\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from scipy.interpolate import griddata\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
        "repoPath = os.path.join(\".\", \"PINNs\")\n",
        "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
        "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
        "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
        "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")\n",
        "\n",
        "sys.path.insert(0, utilsPath)\n",
        "from plotting import newfig, savefig\n",
        "\n",
        "def prep_data(path, N_u=None, N_f=None, N_n=None, q=None, ub=None, lb=None, noise=0.0, idx_t_0=None, idx_t_1=None, N_0=None, N_1=None):\n",
        "    # Reading external data [t is 100x1, usol is 256x100 (solution), x is 256x1]\n",
        "    data = scipy.io.loadmat(path)\n",
        "\n",
        "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
        "    t = data['t'].flatten()[:,None] # T x 1\n",
        "    x = data['x'].flatten()[:,None] # N x 1\n",
        "\n",
        "    # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
        "    Exact_u = np.real(data['usol']).T # T x N\n",
        "\n",
        "    if N_n != None and q != None and ub != None and lb != None and idx_t_0 != None and idx_t_1 != None:\n",
        "      dt = t[idx_t_1] - t[idx_t_0]\n",
        "      idx_x = np.random.choice(Exact_u.shape[1], N_n, replace=False) \n",
        "      x_0 = x[idx_x,:]\n",
        "      u_0 = Exact_u[idx_t_0:idx_t_0+1,idx_x].T\n",
        "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
        "        \n",
        "      # Boudanry data\n",
        "      x_1 = np.vstack((lb, ub))\n",
        "      \n",
        "      # Test data\n",
        "      x_star = x\n",
        "      u_star = Exact_u[idx_t_1,:]\n",
        "\n",
        "      # Load IRK weights\n",
        "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
        "      IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q))\n",
        "      IRK_times = tmp[q**2+q:]\n",
        "\n",
        "      return x, t, dt, Exact_u, x_0, u_0, x_1, x_star, u_star, IRK_weights, IRK_times\n",
        "\n",
        "    # Meshing x and t in 2D (256,100)\n",
        "    X, T = np.meshgrid(x,t)\n",
        "\n",
        "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
        "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "\n",
        "    # Preparing the testing u_star\n",
        "    u_star = Exact_u.flatten()[:,None]\n",
        "                \n",
        "    # Noiseless data TODO: add support for noisy data    \n",
        "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "    X_u_train = X_star[idx,:]\n",
        "    u_train = u_star[idx,:]\n",
        "\n",
        "    if N_0 != None and N_1 != None:\n",
        "      Exact_u = Exact_u.T\n",
        "      idx_x = np.random.choice(Exact_u.shape[0], N_0, replace=False)\n",
        "      x_0 = x[idx_x,:]\n",
        "      u_0 = Exact_u[idx_x,idx_t_0][:,None]\n",
        "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
        "          \n",
        "      idx_x = np.random.choice(Exact_u.shape[0], N_1, replace=False)\n",
        "      x_1 = x[idx_x,:]\n",
        "      u_1 = Exact_u[idx_x,idx_t_1][:,None]\n",
        "      u_1 = u_1 + noise*np.std(u_1)*np.random.randn(u_1.shape[0], u_1.shape[1])\n",
        "      \n",
        "      dt = np.asscalar(t[idx_t_1] - t[idx_t_0])        \n",
        "      q = int(np.ceil(0.5*np.log(np.finfo(float).eps)/np.log(dt)))\n",
        "\n",
        "      # Load IRK weights\n",
        "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
        "      weights =  np.reshape(tmp[0:q**2+q], (q+1,q))     \n",
        "      IRK_alpha = weights[0:-1,:]\n",
        "      IRK_beta = weights[-1:,:] \n",
        "      return x_0, u_0, x_1, u_1, x, t, dt, q, Exact_u, IRK_alpha, IRK_beta\n",
        "\n",
        "    if N_f == None:\n",
        "      lb = X_star.min(axis=0)\n",
        "      ub = X_star.max(axis=0) \n",
        "      return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb\n",
        "\n",
        "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
        "    lb = X_star.min(axis=0)\n",
        "    ub = X_star.max(axis=0) \n",
        "    # Getting the initial conditions (t=0)\n",
        "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "    uu1 = Exact_u[0:1,:].T\n",
        "    # Getting the lowest boundary conditions (x=-1) \n",
        "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "    uu2 = Exact_u[:,0:1]\n",
        "    # Getting the highest boundary conditions (x=1) \n",
        "    xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
        "    uu3 = Exact_u[:,-1:]\n",
        "    # Stacking them in multidimensional tensors for training (X_u_train is for now the continuous boundaries)\n",
        "    X_u_train = np.vstack([xx1, xx2, xx3])\n",
        "    u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "    # Generating the x and t collocation points for f, with each having a N_f size\n",
        "    # We pointwise add and multiply to spread the LHS over the 2D domain\n",
        "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "    # Generating a uniform random sample from ints between 0, and the size of x_u_train, of size N_u (initial data size) and without replacement (unique)\n",
        "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "    # Getting the corresponding X_u_train (which is now scarce boundary/initial coordinates)\n",
        "    X_u_train = X_u_train[idx,:]\n",
        "    # Getting the corresponding u_train\n",
        "    u_train = u_train [idx,:]\n",
        "\n",
        "    return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f_train, ub, lb\n",
        "\n",
        "class Logger(object):\n",
        "  def __init__(self, frequency=10):\n",
        "    print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "    print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
        "\n",
        "    self.start_time = time.time()\n",
        "    self.frequency = frequency\n",
        "\n",
        "  def __get_elapsed(self):\n",
        "    return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
        "\n",
        "  def __get_error_u(self):\n",
        "    return self.error_fn()\n",
        "\n",
        "  def set_error_fn(self, error_fn):\n",
        "    self.error_fn = error_fn\n",
        "  \n",
        "  def log_train_start(self, model):\n",
        "    print(\"\\nTraining started\")\n",
        "    print(\"================\")\n",
        "    self.model = model\n",
        "    print(self.model.summary())\n",
        "\n",
        "  def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
        "    if epoch % self.frequency == 0:\n",
        "      print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
        "\n",
        "  def log_train_opt(self, name):\n",
        "    # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
        "    print(f\"—— Starting {name} optimization ——\")\n",
        "\n",
        "  def log_train_end(self, epoch, custom=\"\"):\n",
        "    print(\"==================\")\n",
        "    print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)\n",
        "\n",
        "def plot_inf_cont_results(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t, file=None):\n",
        "\n",
        "  # Interpolating the results on the whole (x,t) domain.\n",
        "  # griddata(points, values, points at which to interpolate, method)\n",
        "  U_pred = griddata(X_star, u_pred, (X, T), method='cubic')\n",
        "\n",
        "  # Creating the figures\n",
        "  fig, ax = newfig(1.0, 1.1)\n",
        "  ax.axis('off')\n",
        "\n",
        "  ####### Row 0: u(t,x) ##################    \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "\n",
        "  h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
        "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "\n",
        "  ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "\n",
        "  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "  ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
        "\n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  ax.legend(frameon=False, loc = 'best')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "\n",
        "  ####### Row 1: u(t,x) slices ##################    \n",
        "  gs1 = gridspec.GridSpec(1, 3)\n",
        "  gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])\n",
        "  ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 2])\n",
        "  ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])    \n",
        "  ax.set_title('$t = 0.75$', fontsize = 10)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  if file != None:\n",
        "    savefig(file)\n",
        "\n",
        "def plot_inf_disc_results(x_star, idx_t_0, idx_t_1, x_0, u_0, ub, lb, u_1_pred, Exact_u, x, t, file=None):\n",
        "  fig, ax = newfig(1.0, 1.2)\n",
        "  ax.axis('off')\n",
        "  \n",
        "  ####### Row 0: h(t,x) ##################    \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/2 + 0.1, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "  \n",
        "  h = ax.imshow(Exact_u.T, interpolation='nearest', cmap='rainbow', \n",
        "                extent=[t.min(), t.max(), x_star.min(), x_star.max()], \n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "      \n",
        "  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "  ax.plot(t[idx_t_0]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  ax.plot(t[idx_t_1]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  \n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  leg = ax.legend(frameon=False, loc = 'best')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "  \n",
        "  \n",
        "  ####### Row 1: h(t,x) slices ##################    \n",
        "  gs1 = gridspec.GridSpec(1, 2)\n",
        "  gs1.update(top=1-1/2-0.05, bottom=0.15, left=0.15, right=0.85, wspace=0.5)\n",
        "  \n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x,Exact_u[idx_t_0,:], 'b-', linewidth = 2) \n",
        "  ax.plot(x_0, u_0, 'rx', linewidth = 2, label = 'Data')      \n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = %.2f$' % (t[idx_t_0]), fontsize = 10)\n",
        "  ax.set_xlim([lb-0.1, ub+0.1])\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.8, -0.3), ncol=2, frameon=False)\n",
        "\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x, Exact_u[idx_t_1,:], 'b-', linewidth = 2, label = 'Exact') \n",
        "  ax.plot(x_star, u_1_pred, 'r--', linewidth = 2, label = 'Prediction')      \n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = %.2f$' % (t[idx_t_1]), fontsize = 10)    \n",
        "  ax.set_xlim([lb-0.1, ub+0.1])\n",
        "  \n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.1, -0.3), ncol=2, frameon=False)\n",
        "    \n",
        "  plt.show()\n",
        "\n",
        "  if file != None:\n",
        "    savefig(file)\n",
        "\n",
        "\n",
        "def plot_ide_disc_results(x_star, t_star, idx_t_0, idx_t_1, x_0, u_0, x_1, u_1,\n",
        "  ub, lb, u_1_pred, Exact, lambda_1_value, lambda_1_value_noisy, lambda_2_value, lambda_2_value_noisy,\n",
        "  x, t, file=None):  \n",
        "  fig, ax = newfig(1.0, 1.5)\n",
        "  ax.axis('off')\n",
        "  \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/3+0.05, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "      \n",
        "  h = ax.imshow(Exact, interpolation='nearest', cmap='rainbow',\n",
        "                extent=[t_star.min(),t_star.max(), lb[0], ub[0]],\n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "  \n",
        "  line = np.linspace(x_star.min(), x_star.max(), 2)[:,None]\n",
        "  ax.plot(t_star[idx_t_0]*np.ones((2,1)), line, 'w-', linewidth = 1.0)\n",
        "  ax.plot(t_star[idx_t_1]*np.ones((2,1)), line, 'w-', linewidth = 1.0)    \n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "  \n",
        "  gs1 = gridspec.GridSpec(1, 2)\n",
        "  gs1.update(top=1-1/3-0.1, bottom=1-2/3, left=0.15, right=0.85, wspace=0.5)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x_star,Exact[:,idx_t_0][:,None], 'b', linewidth = 2, label = 'Exact')\n",
        "  ax.plot(x_0, u_0, 'rx', linewidth = 2, label = 'Data')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.set_title('$t = %.2f$\\n%d trainng data' % (t_star[idx_t_0], u_0.shape[0]), fontsize = 10)\n",
        "  \n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x_star,Exact[:,idx_t_1][:,None], 'b', linewidth = 2, label = 'Exact')\n",
        "  ax.plot(x_1, u_1, 'rx', linewidth = 2, label = 'Data')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.set_title('$t = %.2f$\\n%d trainng data' % (t_star[idx_t_1], u_1.shape[0]), fontsize = 10)\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(-0.3, -0.3), ncol=2, frameon=False)\n",
        "  \n",
        "  gs2 = gridspec.GridSpec(1, 2)\n",
        "  gs2.update(top=1-2/3-0.05, bottom=0, left=0.15, right=0.85, wspace=0.0)\n",
        "  \n",
        "  ax = plt.subplot(gs2[0, 0])\n",
        "  ax.axis('off')\n",
        "  nu = 0.01/np.pi\n",
        "  s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x + %.6f u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & ' % (nu)\n",
        "  s2 = r'$u_t + %.3f u u_x + %.6f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
        "  s3 = r'Identified PDE (1\\% noise) & '\n",
        "  s4 = r'$u_t + %.3f u u_x + %.6f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
        "  s5 = r'\\end{tabular}$'\n",
        "  s = s1+s2+s3+s4+s5\n",
        "  ax.text(-0.1,0.2,s)\n",
        "  plt.show()\n",
        "\n",
        "def plot_ide_cont_results(X_star, u_pred, X_u_train, u_train,\n",
        "  Exact_u, X, T, x, t, lambda_1_value, lambda_1_value_noisy, lambda_2_value, lambda_2_value_noisy):\n",
        "    fig, ax = newfig(1.0, 1.4)\n",
        "    ax.axis('off')\n",
        "\n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "    \n",
        "    ####### Row 0: u(t,x) ##################    \n",
        "    gs0 = gridspec.GridSpec(1, 2)\n",
        "    gs0.update(top=1-0.06, bottom=1-1.0/3.0+0.06, left=0.15, right=0.85, wspace=0)\n",
        "    ax = plt.subplot(gs0[:, :])\n",
        "    \n",
        "    #h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
        "    #              extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "    #              origin='lower', aspect='auto')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    fig.colorbar(h, cax=cax)\n",
        "    \n",
        "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 2, clip_on = False)\n",
        "    \n",
        "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    \n",
        "    ax.set_xlabel('$t$')\n",
        "    ax.set_ylabel('$x$')\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(1.0, -0.125), ncol=5, frameon=False)\n",
        "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "    \n",
        "    ####### Row 1: u(t,x) slices ##################    \n",
        "    gs1 = gridspec.GridSpec(1, 3)\n",
        "    gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 0])\n",
        "    ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "    ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')    \n",
        "    ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 1])\n",
        "    ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 2])\n",
        "    ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])    \n",
        "    ax.set_title('$t = 0.75$', fontsize = 10)\n",
        "    \n",
        "    ####### Row 3: Identified PDE ##################    \n",
        "    gs2 = gridspec.GridSpec(1, 3)\n",
        "    gs2.update(top=1.0-2.0/3.0, bottom=0, left=0.0, right=1.0, wspace=0.0)\n",
        "    \n",
        "    ax = plt.subplot(gs2[:, :])\n",
        "    ax.axis('off')\n",
        "    s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.0031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\n",
        "    s2 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
        "    s3 = r'Identified PDE (1\\% noise) & '\n",
        "    s4 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
        "    s5 = r'\\end{tabular}$'\n",
        "    s = s1+s2+s3+s4+s5\n",
        "    ax.text(0.1,0.1,s)\n",
        "    plt.show()\n",
        "    # savefig('./figures/Burgers_identification')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7Azv7DZp0M-"
      },
      "source": [
        "custom_lbfgs.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjvMe1Avpvh9",
        "lines_to_next_cell": 0
      },
      "source": [
        "# Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Time tracking functions\n",
        "global_time_list = []\n",
        "global_last_time = 0\n",
        "def reset_time():\n",
        "  global global_time_list, global_last_time\n",
        "  global_time_list = []\n",
        "  global_last_time = time.perf_counter()\n",
        "  \n",
        "def record_time():\n",
        "  global global_last_time, global_time_list\n",
        "  new_time = time.perf_counter()\n",
        "  global_time_list.append(new_time - global_last_time)\n",
        "  global_last_time = time.perf_counter()\n",
        "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
        "\n",
        "def last_time():\n",
        "  \"\"\"Returns last interval records in millis.\"\"\"\n",
        "  global global_last_time, global_time_list\n",
        "  if global_time_list:\n",
        "    return 1000 * global_time_list[-1]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "\n",
        "  if config.maxIter == 0:\n",
        "    return\n",
        "\n",
        "  global final_loss, times\n",
        "  \n",
        "  maxIter = config.maxIter\n",
        "  maxEval = config.maxEval or maxIter*1.25\n",
        "  tolFun = config.tolFun or 1e-5\n",
        "  tolX = config.tolX or 1e-19\n",
        "  nCorrection = config.nCorrection or 100\n",
        "  lineSearch = config.lineSearch\n",
        "  lineSearchOpts = config.lineSearchOptions\n",
        "  learningRate = config.learningRate or 1\n",
        "  isverbose = config.verbose or False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "    # evaluate initial f(x) and df/dx\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "  while nIter < maxIter:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      y = g - g_old\n",
        "      s = d*t\n",
        "      ys = dot(y, s)\n",
        "      \n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian \n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "        \n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "        \n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "    \n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "    # optional line search: user function\n",
        "    lsFuncEval = 0\n",
        "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
        "      # perform line search, using user function\n",
        "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
        "      f_hist.append(f)\n",
        "    else:\n",
        "      # no line search, simply move with fixed-step\n",
        "      x += t*d\n",
        "      \n",
        "      if nIter != maxIter:\n",
        "        # re-evaluate function only if not in last iteration\n",
        "        # the reason we do this: in a stochastic setting,\n",
        "        # no use to re-evaluate that function here\n",
        "        f, g = opfunc(x)\n",
        "        lsFuncEval = 1\n",
        "        f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      verbose('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      verbose('optimality condition below tolFun')\n",
        "      break\n",
        "    \n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      verbose('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f-f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if do_verbose:\n",
        "      log_fn(nIter, f.numpy(), True)\n",
        "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
        "      record_time()\n",
        "      times.append(last_time())\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOT-E8C4oAJN"
      },
      "source": [
        "# 1. Continuous Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qrt3ECzcLHp"
      },
      "source": [
        "$$u_t + u u_x - \\nu u_{xx} = 0$$\n",
        "\n",
        "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
        "\n",
        "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8CHqrpafela"
      },
      "source": [
        "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
        "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
        "\n",
        "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
        "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
        "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Ko6L87J2v_"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwWhiecUqbAo",
        "lines_to_next_cell": 2
      },
      "source": [
        "\n",
        "# Data size on the solution u\n",
        "N_u = 50\n",
        "# Collocation points size, where we’ll check for f = 0\n",
        "N_f = 10000\n",
        "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 100\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  learning_rate=0.1,\n",
        "  beta_1=0.99,\n",
        "  epsilon=1e-1)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 2000\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 0.8\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 50\n",
        "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkimJNtepkKi"
      },
      "source": [
        "## PINN class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3wUjV9oe7V9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVm9UCvvlyY_"
      },
      "source": [
        "\n",
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu):\n",
        "    # Descriptive Keras model [2, 20, …, 20, 1]\n",
        "    self.u_model = tf.keras.Sequential()\n",
        "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.u_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:]:\n",
        "        self.u_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.nu = nu\n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    # Separating the collocation coordinates\n",
        "    self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
        "    self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
        "    \n",
        "  # Defining custom loss\n",
        "  def __loss(self, u, u_pred):\n",
        "    f_pred = self.f_model()\n",
        "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
        "      tf.reduce_mean(tf.square(f_pred))\n",
        "\n",
        "  def __grad(self, X, u):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(u, self.u_model(X))\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.u_model.trainable_variables\n",
        "    return var\n",
        "\n",
        "  # The actual PINN\n",
        "  def f_model(self):\n",
        "    # Using the new GradientTape paradigm of TF2.0,\n",
        "    # which keeps track of operations to get the gradient at runtime\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Watching the two inputs we’ll need later, x and t\n",
        "      tape.watch(self.x_f)\n",
        "      tape.watch(self.t_f)\n",
        "      # Packing together the inputs\n",
        "      X_f = tf.stack([self.x_f[:,0], self.t_f[:,0]], axis=1)\n",
        "\n",
        "      # Getting the prediction\n",
        "      u = self.u_model(X_f)\n",
        "      # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
        "      u_x = tape.gradient(u, self.x_f)\n",
        "    \n",
        "    # Getting the other derivatives\n",
        "    u_xx = tape.gradient(u_x, self.x_f)\n",
        "    u_t = tape.gradient(u, self.t_f)\n",
        "\n",
        "    # Letting the tape go\n",
        "    del tape\n",
        "\n",
        "    nu = self.get_params(numpy=True)\n",
        "\n",
        "    # Buidling the PINNs\n",
        "    return u_t + u*u_x - nu*u_xx\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    return self.nu\n",
        "\n",
        "  def get_weights(self):\n",
        "    w = []\n",
        "    for layer in self.u_model.layers[1:]:\n",
        "      weights_biases = layer.get_weights()\n",
        "      weights = weights_biases[0].flatten()\n",
        "      biases = weights_biases[1]\n",
        "      w.extend(weights)\n",
        "      w.extend(biases)\n",
        "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "\n",
        "  def summary(self):\n",
        "    return self.u_model.summary()\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
        "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(X_u, u)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
        "      self.logger.log_train_epoch(epoch, loss_value)\n",
        "    \n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        loss_value = self.__loss(u, self.u_model(X_u))\n",
        "      grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "    # tfp.optimizer.lbfgs_minimize(\n",
        "    #   loss_and_flat_grad,\n",
        "    #   initial_position=self.get_weights(),\n",
        "    #   num_correction_pairs=nt_config.nCorrection,\n",
        "    #   max_iterations=nt_config.maxIter,\n",
        "    #   f_relative_tolerance=nt_config.tolFun,\n",
        "    #   tolerance=nt_config.tolFun,\n",
        "    #   parallel_iterations=6)\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights(),\n",
        "      nt_config, Struct(), True,\n",
        "      lambda epoch, loss, is_iter:\n",
        "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
        "\n",
        "    self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
        "\n",
        "  def predict(self, X_star):\n",
        "    u_star = self.u_model(X_star)\n",
        "    f_star = self.f_model()\n",
        "    return u_star, f_star"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FzMd65dpoHo"
      },
      "source": [
        "## Training and plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEKkpHvApf46",
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4572438-4164-4257-b587-e135f1527e18"
      },
      "source": [
        "\n",
        "# Getting the data\n",
        "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
        "x, t, X, T, Exact_u, X_star, u_star, \\\n",
        "  X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
        "\n",
        "# Creating the model and training\n",
        "logger = Logger(frequency=10)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb, nu=0.01/np.pi)\n",
        "def error():\n",
        "  u_pred, _ = pinn.predict(X_star)\n",
        "  return np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_pred, f_pred = pinn.predict(X_star)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.9.1\n",
            "Eager execution: True\n",
            "GPU-accerelated: True\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_1 (Lambda)           (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 20)                60        \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,021\n",
            "Trainable params: 3,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "tf_epoch =      0  elapsed = 00:00  loss = 2.9103e-01  error = 8.8958e-01  \n",
            "tf_epoch =     10  elapsed = 00:00  loss = 2.1053e-01  error = 7.8298e-01  \n",
            "tf_epoch =     20  elapsed = 00:01  loss = 1.8826e-01  error = 6.7527e-01  \n",
            "tf_epoch =     30  elapsed = 00:02  loss = 1.6335e-01  error = 6.6386e-01  \n",
            "tf_epoch =     40  elapsed = 00:02  loss = 1.5234e-01  error = 6.1549e-01  \n",
            "tf_epoch =     50  elapsed = 00:03  loss = 1.5271e-01  error = 5.5059e-01  \n",
            "tf_epoch =     60  elapsed = 00:04  loss = 1.5121e-01  error = 6.1824e-01  \n",
            "tf_epoch =     70  elapsed = 00:06  loss = 1.4983e-01  error = 5.9295e-01  \n",
            "tf_epoch =     80  elapsed = 00:07  loss = 1.4985e-01  error = 6.3547e-01  \n",
            "tf_epoch =     90  elapsed = 00:08  loss = 1.4213e-01  error = 5.9022e-01  \n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 00:11  loss = 1.0505e-01  error = 5.0906e-01  \n",
            "nt_epoch =     20  elapsed = 00:13  loss = 8.8612e-02  error = 5.1689e-01  \n",
            "nt_epoch =     30  elapsed = 00:15  loss = 7.3399e-02  error = 4.9323e-01  \n",
            "nt_epoch =     40  elapsed = 00:18  loss = 6.4590e-02  error = 4.6721e-01  \n",
            "nt_epoch =     50  elapsed = 00:20  loss = 6.1805e-02  error = 4.5981e-01  \n",
            "nt_epoch =     60  elapsed = 00:22  loss = 5.7593e-02  error = 4.4954e-01  \n",
            "nt_epoch =     70  elapsed = 00:25  loss = 5.4635e-02  error = 4.4226e-01  \n",
            "nt_epoch =     80  elapsed = 00:26  loss = 4.9311e-02  error = 4.2250e-01  \n",
            "nt_epoch =     90  elapsed = 00:27  loss = 4.4712e-02  error = 3.8744e-01  \n",
            "nt_epoch =    100  elapsed = 00:28  loss = 4.0963e-02  error = 3.6294e-01  \n",
            "==================\n",
            "Training finished (epoch 2100): duration = 00:28  error = 1.9818e+00  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_star.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A5mUrwivvlB",
        "outputId": "08121f34-a505-49d8-f621-cf32e3dc7161"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25600, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u_pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXslWmylv6Ga",
        "outputId": "a0f9f79c-75dc-47de-e270-afb1560c1eef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([25600, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf1QXaK5PlUh",
        "lines_to_next_cell": 0
      },
      "source": [
        "#plot_inf_cont_results(X_star, u_pred, X_u_train, u_train,Exact_u, X, T, x, t)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-Dt42ItxAqr"
      },
      "source": [
        "# 2. Discrete Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1oPFjYDxAqs"
      },
      "source": [
        "$$u_t + u u_x - \\nu u_{xx} = 0$$\n",
        "\n",
        "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
        "\n",
        "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4z7cQz_xAqt"
      },
      "source": [
        "Approximating $u(t,x)$ with a deep NN, we define the PINN: (TODO)\n",
        "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
        "\n",
        "We train the shared parameters between the deep NN and the PINN minimizing the loss: (TODO)\n",
        "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
        "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGd5zVtoxAqt"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKrkV1ChxAqu",
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea00a4ed-bd90-450f-ad41-cbe75b4fedf9"
      },
      "source": [
        "\n",
        "# Data size on initial condition on u\n",
        "N_n = 250\n",
        "# Number of RK stages\n",
        "q = 500\n",
        "# DeepNN topology (1-sized input [x], 3 hidden layer of 50-width, q+1-sized output [u_1^n(x), ..., u_{q+1}^n(x)]\n",
        "layers = [1, 50, 50, 50, q + 1]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 200\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  lr=0.001,\n",
        "  beta_1=0.9,\n",
        "  beta_2=0.999,\n",
        "  epsilon=1e-08)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 1000\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 0.8\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 50\n",
        "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXtJ5GiaxAqw"
      },
      "source": [
        "## PINN class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiPSoTyPxAqw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2acHqmorxAqx"
      },
      "source": [
        "\n",
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, dt, x_1, lb, ub, nu, q, IRK_weights, IRK_times):\n",
        "    self.lb = lb\n",
        "    self.ub = ub\n",
        "    self.nu = nu\n",
        "\n",
        "    self.dt = dt\n",
        "\n",
        "    self.q = max(q,1)\n",
        "    self.IRK_weights = IRK_weights\n",
        "    self.IRK_times = IRK_times\n",
        "\n",
        "    # Descriptive Keras model [2, 50, …, 50, q+1]\n",
        "    self.U_1_model = tf.keras.Sequential()\n",
        "    self.U_1_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.U_1_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:]:\n",
        "        self.U_1_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    self.x_1 = tf.convert_to_tensor(x_1, dtype=self.dtype)\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "  def U_0_model(self, x):\n",
        "    # Using the new GradientTape paradigm of TF2.0,\n",
        "    # which keeps track of operations to get the gradient at runtime\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Watching the two inputs we’ll need later, x and t\n",
        "      tape.watch(x)\n",
        "      tape.watch(self.dummy_x0_tf)\n",
        "\n",
        "      # Getting the prediction, and removing the last item (q+1)\n",
        "      U_1 = self.U_1_model(x) # shape=(len(x), q+1)\n",
        "      U = U_1[:, :-1] # shape=(len(x), q)\n",
        "\n",
        "      # Deriving INSIDE the tape (2-step-dummy grad technique because U is a mat)\n",
        "      g_U = tape.gradient(U, x, output_gradients=self.dummy_x0_tf)\n",
        "      U_x = tape.gradient(g_U, self.dummy_x0_tf)\n",
        "      g_U_x = tape.gradient(U_x, x, output_gradients=self.dummy_x0_tf)\n",
        "    \n",
        "    # Doing the last one outside the with, to optimize performance\n",
        "    # Impossible to do for the earlier grad, because they’re needed after\n",
        "    U_xx = tape.gradient(g_U_x, self.dummy_x0_tf)\n",
        "\n",
        "    # Letting the tape go\n",
        "    del tape\n",
        "\n",
        "    # Buidling the PINNs, shape = (len(x), q+1), IRK shape = (q, q+1)\n",
        "    nu = self.get_params(numpy=True)\n",
        "    N = U*U_x - nu*U_xx # shape=(len(x), q)\n",
        "    return U_1 + self.dt*tf.matmul(N, self.IRK_weights.T)\n",
        "\n",
        "  # Defining custom loss\n",
        "  def __loss(self, u_0, u_0_pred):\n",
        "    u_1_pred = self.U_1_model(self.x_1)\n",
        "    return tf.reduce_sum(tf.square(u_0_pred - u_0)) + \\\n",
        "      tf.reduce_sum(tf.square(u_1_pred))\n",
        "\n",
        "  def __grad(self, x_0, u_0):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(u_0, self.U_0_model(x_0))\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.U_1_model.trainable_variables\n",
        "    return var\n",
        "\n",
        "  def get_weights(self):\n",
        "    w = []\n",
        "    for layer in self.U_1_model.layers[1:]:\n",
        "      weights_biases = layer.get_weights()\n",
        "      weights = weights_biases[0].flatten()\n",
        "      biases = weights_biases[1]\n",
        "      w.extend(weights)\n",
        "      w.extend(biases)\n",
        "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.U_1_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    return self.nu\n",
        "\n",
        "  def summary(self):\n",
        "    return self.U_1_model.summary()\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, x_0, u_0, tf_epochs, nt_config):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    x_0 = tf.convert_to_tensor(x_0, dtype=self.dtype)\n",
        "    u_0 = tf.convert_to_tensor(u_0, dtype=self.dtype)\n",
        "\n",
        "    # Creating dummy tensors for the gradients\n",
        "    self.dummy_x0_tf = tf.ones([x_0.shape[0], self.q], dtype=self.dtype)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(x_0, u_0)\n",
        "      self.optimizer.apply_gradients(\n",
        "        zip(grads, self.__wrap_training_variables()))\n",
        "      self.logger.log_train_epoch(epoch, loss_value)\n",
        "\n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        loss_value = self.__loss(u_0, self.U_0_model(x_0))\n",
        "      grad = tape.gradient(loss_value, self.U_1_model.trainable_variables)\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "    # tfp.optimizer.lbfgs_minimize(\n",
        "    #   loss_and_flat_grad,\n",
        "    #   initial_position=self.get_weights(),\n",
        "    #   num_correction_pairs=nt_config.nCorrection,\n",
        "    #   max_iterations=nt_config.maxIter,\n",
        "    #   f_relative_tolerance=nt_config.tolFun,\n",
        "    #   tolerance=nt_config.tolFun,\n",
        "    #   parallel_iterations=6)\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights(),\n",
        "      nt_config, Struct(), True,\n",
        "      lambda epoch, loss, is_iter:\n",
        "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
        "    \n",
        "    self.logger.log_train_end(tf_epochs)\n",
        "\n",
        "  def predict(self, x_star):\n",
        "    u_star = self.U_1_model(x_star)[:, -1]\n",
        "    return u_star"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIq8U_a_xAqy"
      },
      "source": [
        "## Training and plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4ICbcZhxAqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab1642a-8239-475f-ecd6-0d7f25692da2"
      },
      "source": [
        "\n",
        "# Setup\n",
        "lb = np.array([-1.0])\n",
        "ub = np.array([1.0])\n",
        "idx_t_0 = 10\n",
        "idx_t_1 = 90\n",
        "nu = 0.01/np.pi\n",
        "\n",
        "# Getting the data\n",
        "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
        "x, t, dt, \\\n",
        "  Exact_u, x_0, u_0, x_1, x_star, u_star, \\\n",
        "  IRK_weights, IRK_times = prep_data(path, N_n=N_n, q=q, lb=lb, ub=ub, noise=0.0, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
        "\n",
        "# Creating the model and training\n",
        "logger = Logger(frequency=10)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, x_1, lb, ub, nu, q, IRK_weights, IRK_times)\n",
        "def error():\n",
        "  u_pred = pinn.predict(x_star)\n",
        "  return np.linalg.norm(u_pred - u_star, 2) / np.linalg.norm(u_star, 2)\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(x_0, u_0, tf_epochs, nt_config)\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_1_pred = pinn.predict(x_star)\n",
        "\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_1_pred = pinn.predict(x_star)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.9.1\n",
            "Eager execution: True\n",
            "GPU-accerelated: True\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_2 (Lambda)           (None, 1)                 0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 50)                100       \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 50)                2550      \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 50)                2550      \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 501)               25551     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,751\n",
            "Trainable params: 30,751\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "tf_epoch =      0  elapsed = 00:00  loss = 6.1188e+04  error = 9.9183e-01  \n",
            "tf_epoch =     10  elapsed = 00:00  loss = 5.2999e+04  error = 9.1168e-01  \n",
            "tf_epoch =     20  elapsed = 00:00  loss = 4.0422e+04  error = 8.3942e-01  \n",
            "tf_epoch =     30  elapsed = 00:01  loss = 3.0798e+04  error = 1.0075e+00  \n",
            "tf_epoch =     40  elapsed = 00:01  loss = 2.6585e+04  error = 1.2057e+00  \n",
            "tf_epoch =     50  elapsed = 00:02  loss = 2.3989e+04  error = 1.3011e+00  \n",
            "tf_epoch =     60  elapsed = 00:02  loss = 2.1985e+04  error = 1.3448e+00  \n",
            "tf_epoch =     70  elapsed = 00:02  loss = 2.0261e+04  error = 1.3670e+00  \n",
            "tf_epoch =     80  elapsed = 00:03  loss = 1.8731e+04  error = 1.3788e+00  \n",
            "tf_epoch =     90  elapsed = 00:03  loss = 1.7442e+04  error = 1.3852e+00  \n",
            "tf_epoch =    100  elapsed = 00:04  loss = 1.6428e+04  error = 1.3889e+00  \n",
            "tf_epoch =    110  elapsed = 00:04  loss = 1.5687e+04  error = 1.3913e+00  \n",
            "tf_epoch =    120  elapsed = 00:04  loss = 1.5193e+04  error = 1.3929e+00  \n",
            "tf_epoch =    130  elapsed = 00:05  loss = 1.4885e+04  error = 1.3941e+00  \n",
            "tf_epoch =    140  elapsed = 00:05  loss = 1.4671e+04  error = 1.3951e+00  \n",
            "tf_epoch =    150  elapsed = 00:05  loss = 1.4462e+04  error = 1.3959e+00  \n",
            "tf_epoch =    160  elapsed = 00:06  loss = 1.4274e+04  error = 1.3966e+00  \n",
            "tf_epoch =    170  elapsed = 00:06  loss = 1.4054e+04  error = 1.3972e+00  \n",
            "tf_epoch =    180  elapsed = 00:07  loss = 1.3774e+04  error = 1.3978e+00  \n",
            "tf_epoch =    190  elapsed = 00:07  loss = 1.3375e+04  error = 1.3985e+00  \n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 00:08  loss = 1.0259e+04  error = 1.3989e+00  \n",
            "nt_epoch =     20  elapsed = 00:08  loss = 7.9918e+03  error = 1.3987e+00  \n",
            "nt_epoch =     30  elapsed = 00:09  loss = 6.6366e+03  error = 1.3958e+00  \n",
            "nt_epoch =     40  elapsed = 00:10  loss = 5.2849e+03  error = 1.3820e+00  \n",
            "nt_epoch =     50  elapsed = 00:10  loss = 4.2261e+03  error = 1.3177e+00  \n",
            "nt_epoch =     60  elapsed = 00:11  loss = 2.9835e+03  error = 5.9447e-01  \n",
            "nt_epoch =     70  elapsed = 00:12  loss = 2.2515e+03  error = 2.3473e-01  \n",
            "nt_epoch =     80  elapsed = 00:13  loss = 1.7911e+03  error = 1.7154e-01  \n",
            "nt_epoch =     90  elapsed = 00:13  loss = 1.2042e+03  error = 2.6800e-01  \n",
            "nt_epoch =    100  elapsed = 00:14  loss = 6.7208e+02  error = 2.4528e-01  \n",
            "nt_epoch =    110  elapsed = 00:15  loss = 5.1530e+02  error = 2.2854e-01  \n",
            "nt_epoch =    120  elapsed = 00:16  loss = 4.2907e+02  error = 1.9298e-01  \n",
            "nt_epoch =    130  elapsed = 00:16  loss = 3.7461e+02  error = 1.6802e-01  \n",
            "nt_epoch =    140  elapsed = 00:17  loss = 3.9171e+02  error = 1.9009e-01  \n",
            "nt_epoch =    150  elapsed = 00:18  loss = 2.7153e+02  error = 1.6276e-01  \n",
            "nt_epoch =    160  elapsed = 00:19  loss = 1.9765e+02  error = 1.5891e-01  \n",
            "nt_epoch =    170  elapsed = 00:19  loss = 1.6288e+02  error = 1.6093e-01  \n",
            "nt_epoch =    180  elapsed = 00:20  loss = 1.2876e+02  error = 1.5719e-01  \n",
            "nt_epoch =    190  elapsed = 00:21  loss = 1.1567e+02  error = 1.5353e-01  \n",
            "nt_epoch =    200  elapsed = 00:22  loss = 1.0725e+02  error = 1.4453e-01  \n",
            "nt_epoch =    210  elapsed = 00:22  loss = 9.9530e+01  error = 1.3360e-01  \n",
            "nt_epoch =    220  elapsed = 00:23  loss = 9.5876e+01  error = 1.3347e-01  \n",
            "nt_epoch =    230  elapsed = 00:24  loss = 9.0604e+01  error = 1.3401e-01  \n",
            "nt_epoch =    240  elapsed = 00:25  loss = 8.2676e+01  error = 1.2943e-01  \n",
            "nt_epoch =    250  elapsed = 00:26  loss = 7.9283e+01  error = 1.2633e-01  \n",
            "nt_epoch =    260  elapsed = 00:27  loss = 7.3229e+01  error = 1.2035e-01  \n",
            "nt_epoch =    270  elapsed = 00:27  loss = 6.8683e+01  error = 1.1301e-01  \n",
            "nt_epoch =    280  elapsed = 00:28  loss = 6.4039e+01  error = 1.0660e-01  \n",
            "nt_epoch =    290  elapsed = 00:29  loss = 6.1508e+01  error = 1.0343e-01  \n",
            "nt_epoch =    300  elapsed = 00:30  loss = 5.9097e+01  error = 1.0002e-01  \n",
            "nt_epoch =    310  elapsed = 00:30  loss = 5.7450e+01  error = 9.8472e-02  \n",
            "nt_epoch =    320  elapsed = 00:31  loss = 5.4403e+01  error = 9.5722e-02  \n",
            "nt_epoch =    330  elapsed = 00:32  loss = 5.1912e+01  error = 9.4592e-02  \n",
            "nt_epoch =    340  elapsed = 00:33  loss = 5.0485e+01  error = 9.2607e-02  \n",
            "nt_epoch =    350  elapsed = 00:34  loss = 4.8272e+01  error = 9.0634e-02  \n",
            "nt_epoch =    360  elapsed = 00:34  loss = 4.7048e+01  error = 8.6530e-02  \n",
            "nt_epoch =    370  elapsed = 00:35  loss = 4.5179e+01  error = 8.2726e-02  \n",
            "nt_epoch =    380  elapsed = 00:36  loss = 4.2983e+01  error = 7.9658e-02  \n",
            "nt_epoch =    390  elapsed = 00:37  loss = 4.1601e+01  error = 7.9512e-02  \n",
            "nt_epoch =    400  elapsed = 00:37  loss = 4.0511e+01  error = 7.9361e-02  \n",
            "nt_epoch =    410  elapsed = 00:38  loss = 3.9036e+01  error = 7.7742e-02  \n",
            "nt_epoch =    420  elapsed = 00:39  loss = 3.7342e+01  error = 7.7871e-02  \n",
            "nt_epoch =    430  elapsed = 00:40  loss = 3.5710e+01  error = 7.7691e-02  \n",
            "nt_epoch =    440  elapsed = 00:40  loss = 3.4306e+01  error = 7.6207e-02  \n",
            "nt_epoch =    450  elapsed = 00:41  loss = 3.3195e+01  error = 7.4575e-02  \n",
            "nt_epoch =    460  elapsed = 00:42  loss = 3.2269e+01  error = 7.5085e-02  \n",
            "nt_epoch =    470  elapsed = 00:43  loss = 3.1623e+01  error = 7.5950e-02  \n",
            "nt_epoch =    480  elapsed = 00:43  loss = 3.0146e+01  error = 7.6443e-02  \n",
            "nt_epoch =    490  elapsed = 00:44  loss = 2.9241e+01  error = 7.6903e-02  \n",
            "nt_epoch =    500  elapsed = 00:45  loss = 2.8398e+01  error = 7.8220e-02  \n",
            "nt_epoch =    510  elapsed = 00:46  loss = 2.7783e+01  error = 7.9155e-02  \n",
            "nt_epoch =    520  elapsed = 00:46  loss = 2.6946e+01  error = 7.9723e-02  \n",
            "nt_epoch =    530  elapsed = 00:47  loss = 2.5997e+01  error = 7.8114e-02  \n",
            "nt_epoch =    540  elapsed = 00:48  loss = 2.5337e+01  error = 7.8084e-02  \n",
            "nt_epoch =    550  elapsed = 00:49  loss = 2.4660e+01  error = 7.6789e-02  \n",
            "nt_epoch =    560  elapsed = 00:49  loss = 2.4002e+01  error = 7.6557e-02  \n",
            "nt_epoch =    570  elapsed = 00:50  loss = 2.3253e+01  error = 7.6844e-02  \n",
            "nt_epoch =    580  elapsed = 00:51  loss = 2.2515e+01  error = 7.5489e-02  \n",
            "nt_epoch =    590  elapsed = 00:52  loss = 2.1719e+01  error = 7.3281e-02  \n",
            "nt_epoch =    600  elapsed = 00:52  loss = 2.1151e+01  error = 6.9611e-02  \n",
            "nt_epoch =    610  elapsed = 00:53  loss = 2.0468e+01  error = 6.9054e-02  \n",
            "nt_epoch =    620  elapsed = 00:54  loss = 1.9939e+01  error = 6.7663e-02  \n",
            "nt_epoch =    630  elapsed = 00:55  loss = 1.9400e+01  error = 6.5370e-02  \n",
            "nt_epoch =    640  elapsed = 00:56  loss = 1.8821e+01  error = 6.3706e-02  \n",
            "nt_epoch =    650  elapsed = 00:56  loss = 1.8224e+01  error = 6.3274e-02  \n",
            "nt_epoch =    660  elapsed = 00:57  loss = 1.7692e+01  error = 6.3199e-02  \n",
            "nt_epoch =    670  elapsed = 00:58  loss = 1.7330e+01  error = 6.3226e-02  \n",
            "nt_epoch =    680  elapsed = 00:59  loss = 1.6842e+01  error = 6.3037e-02  \n",
            "nt_epoch =    690  elapsed = 00:59  loss = 1.6432e+01  error = 6.2712e-02  \n",
            "nt_epoch =    700  elapsed = 01:00  loss = 1.5905e+01  error = 6.2995e-02  \n",
            "nt_epoch =    710  elapsed = 01:01  loss = 1.5606e+01  error = 6.2088e-02  \n",
            "nt_epoch =    720  elapsed = 01:02  loss = 1.5377e+01  error = 6.2584e-02  \n",
            "nt_epoch =    730  elapsed = 01:02  loss = 1.5065e+01  error = 6.2312e-02  \n",
            "nt_epoch =    740  elapsed = 01:03  loss = 1.4731e+01  error = 6.3212e-02  \n",
            "nt_epoch =    750  elapsed = 01:04  loss = 1.4249e+01  error = 6.5080e-02  \n",
            "nt_epoch =    760  elapsed = 01:05  loss = 1.3940e+01  error = 6.5978e-02  \n",
            "nt_epoch =    770  elapsed = 01:05  loss = 1.3635e+01  error = 6.7324e-02  \n",
            "nt_epoch =    780  elapsed = 01:06  loss = 1.3429e+01  error = 6.8666e-02  \n",
            "nt_epoch =    790  elapsed = 01:07  loss = 1.3240e+01  error = 7.0135e-02  \n",
            "nt_epoch =    800  elapsed = 01:08  loss = 1.2982e+01  error = 7.1287e-02  \n",
            "nt_epoch =    810  elapsed = 01:09  loss = 1.2714e+01  error = 7.0703e-02  \n",
            "nt_epoch =    820  elapsed = 01:09  loss = 1.2434e+01  error = 6.9209e-02  \n",
            "nt_epoch =    830  elapsed = 01:10  loss = 1.2223e+01  error = 6.9058e-02  \n",
            "nt_epoch =    840  elapsed = 01:11  loss = 1.1979e+01  error = 6.8741e-02  \n",
            "nt_epoch =    850  elapsed = 01:12  loss = 1.1800e+01  error = 6.9510e-02  \n",
            "nt_epoch =    860  elapsed = 01:12  loss = 1.1634e+01  error = 6.9654e-02  \n",
            "nt_epoch =    870  elapsed = 01:13  loss = 1.1477e+01  error = 7.0354e-02  \n",
            "nt_epoch =    880  elapsed = 01:14  loss = 1.1333e+01  error = 7.0262e-02  \n",
            "nt_epoch =    890  elapsed = 01:15  loss = 1.1156e+01  error = 7.0025e-02  \n",
            "nt_epoch =    900  elapsed = 01:15  loss = 1.0870e+01  error = 6.8400e-02  \n",
            "nt_epoch =    910  elapsed = 01:16  loss = 1.0711e+01  error = 6.8455e-02  \n",
            "nt_epoch =    920  elapsed = 01:17  loss = 1.0529e+01  error = 6.7844e-02  \n",
            "nt_epoch =    930  elapsed = 01:18  loss = 1.1018e+01  error = 6.9837e-02  \n",
            "nt_epoch =    940  elapsed = 01:19  loss = 1.0196e+01  error = 6.8320e-02  \n",
            "nt_epoch =    950  elapsed = 01:19  loss = 1.0194e+01  error = 6.8110e-02  \n",
            "nt_epoch =    960  elapsed = 01:20  loss = 9.8859e+00  error = 6.9328e-02  \n",
            "nt_epoch =    970  elapsed = 01:21  loss = 9.5894e+00  error = 6.9660e-02  \n",
            "nt_epoch =    980  elapsed = 01:22  loss = 9.4914e+00  error = 6.8205e-02  \n",
            "nt_epoch =    990  elapsed = 01:22  loss = 9.3887e+00  error = 6.8121e-02  \n",
            "==================\n",
            "Training finished (epoch 200): duration = 01:23  error = 6.8255e-02  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9aFDcGpxAq2",
        "lines_to_next_cell": 0
      },
      "source": [
        "#plot_inf_disc_results(x_star, idx_t_0, idx_t_1, x_0, u_0, ub, lb, u_1_pred, Exact_u, x, t)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoN7tCmSGDeO"
      },
      "source": [
        "# 3. Continuous Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkpuORIsGDeQ"
      },
      "source": [
        "$$u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx} = 0$$\n",
        "\n",
        "With $\\lambda_1$ and $\\lambda_2$ real parameters of the differential operator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpUoecjeGDeR"
      },
      "source": [
        "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
        "$$f := u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx}.$$\n",
        "\n",
        "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
        "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_u}|f(t_u^i,x_u^i)|^2,$$\n",
        "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ respectively the trainring data on $u(t,x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOPzdkKsJzA4"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKRYhyXqGDeT",
        "lines_to_next_cell": 2
      },
      "source": [
        "\n",
        "# Data size on the solution u\n",
        "N_u = 2000\n",
        "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 100\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  learning_rate=0.001)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 1000\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 0.8\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 50\n",
        "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTxvp1nJGDeb"
      },
      "source": [
        "## PINN class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9grEA3wGDed"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLOwP1UfGDee",
        "lines_to_next_cell": 2
      },
      "source": [
        "\n",
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, ub, lb):\n",
        "    # Descriptive Keras model [2, 20, …, 20, 1]\n",
        "    self.u_model = tf.keras.Sequential()\n",
        "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.u_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:]:\n",
        "        self.u_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    # Defining the two additional trainable variables for identification\n",
        "    self.lambda_1 = tf.Variable([0.0], dtype=self.dtype)\n",
        "    self.lambda_2 = tf.Variable([-6.0], dtype=self.dtype)\n",
        "    \n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "  # The actual PINN\n",
        "  def __f_model(self, X_u):\n",
        "    l1, l2 = self.get_params()\n",
        "    # Separating the collocation coordinates\n",
        "    x_f = tf.convert_to_tensor(X_u[:, 0:1], dtype=self.dtype)\n",
        "    t_f = tf.convert_to_tensor(X_u[:, 1:2], dtype=self.dtype)\n",
        "\n",
        "    # Using the new GradientTape paradigm of TF2.0,\n",
        "    # which keeps track of operations to get the gradient at runtime\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Watching the two inputs we’ll need later, x and t\n",
        "      tape.watch(x_f)\n",
        "      tape.watch(t_f)\n",
        "      # Packing together the inputs\n",
        "      X_f = tf.stack([x_f[:,0], t_f[:,0]], axis=1)\n",
        "\n",
        "\n",
        "      # Getting the prediction\n",
        "      u = self.u_model(X_f)\n",
        "      # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
        "      u_x = tape.gradient(u, x_f)\n",
        "    \n",
        "    # Getting the other derivatives\n",
        "    u_xx = tape.gradient(u_x, x_f)\n",
        "    u_t = tape.gradient(u, t_f)\n",
        "\n",
        "    # Letting the tape go\n",
        "    del tape\n",
        "\n",
        "    # Buidling the PINNs\n",
        "    return u_t + l1*u*u_x - l2*u_xx\n",
        "\n",
        "  # Defining custom loss\n",
        "  def __loss(self, X_u, u, u_pred):\n",
        "    f_pred = self.__f_model(X_u)\n",
        "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
        "      tf.reduce_mean(tf.square(f_pred))\n",
        "\n",
        "  def __grad(self, X, u):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(X, u, self.u_model(X))\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.u_model.trainable_variables\n",
        "    var.extend([self.lambda_1, self.lambda_2])\n",
        "    return var\n",
        "\n",
        "  def get_weights(self):\n",
        "      w = []\n",
        "      for layer in self.u_model.layers[1:]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "      w.extend(self.lambda_1.numpy())\n",
        "      w.extend(self.lambda_2.numpy())\n",
        "      return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "    self.lambda_1.assign([w[-2]])\n",
        "    self.lambda_2.assign([w[-1]])\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    l1 = self.lambda_1\n",
        "    l2 = tf.exp(self.lambda_2)\n",
        "    if numpy:\n",
        "      return l1.numpy()[0], l2.numpy()[0]\n",
        "    return l1, l2\n",
        "\n",
        "  def summary(self):\n",
        "    return self.u_model.summary()\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, X_u, u, tf_epochs, nt_config):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
        "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
        "\n",
        "    def log_train_epoch(epoch, loss, is_iter):\n",
        "      l1, l2 = self.get_params(numpy=True)\n",
        "      custom = f\"l1 = {l1:5f}  l2 = {l2:8f}\"\n",
        "      self.logger.log_train_epoch(epoch, loss, custom, is_iter)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(X_u, u)\n",
        "      self.optimizer.apply_gradients(\n",
        "        zip(grads, self.__wrap_training_variables()))\n",
        "      log_train_epoch(epoch, loss_value, False)\n",
        "\n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        tape.watch(self.lambda_1)\n",
        "        tape.watch(self.lambda_2)\n",
        "        loss_value = self.__loss(X_u, u, self.u_model(X_u))\n",
        "      grad = tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "    # tfp.optimizer.lbfgs_minimize(\n",
        "    #   loss_and_flat_grad,\n",
        "    #   initial_position=self.get_weights(),\n",
        "    #   num_correction_pairs=nt_config.nCorrection,\n",
        "    #   max_iterations=nt_config.maxIter,\n",
        "    #   f_relative_tolerance=nt_config.tolFun,\n",
        "    #   tolerance=nt_config.tolFun,\n",
        "    #   parallel_iterations=6)\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights(),\n",
        "      nt_config, Struct(), True, log_train_epoch)\n",
        "    \n",
        "    l1, l2 = self.get_params(numpy=True)\n",
        "    self.logger.log_train_end(tf_epochs, f\"l1 = {l1:5f}  l2 = {l2:8f}\")\n",
        "\n",
        "  def predict(self, X_star):\n",
        "    u_star = self.u_model(X_star)\n",
        "    f_star = self.__f_model(X_star)\n",
        "    return u_star.numpy(), f_star.numpy()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rWEI708GDei"
      },
      "source": [
        "## Training and plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxkeBW46GDek",
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720e8abe-6616-44cf-e4b5-9bb83eb726d2"
      },
      "source": [
        "\n",
        "# Getting the data\n",
        "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
        "x, t, X, T, Exact_u, X_star, u_star, \\\n",
        "  X_u_train, u_train, ub, lb = prep_data(path, N_u, noise=0.0)\n",
        "lambdas_star = (1.0, 0.01/np.pi)\n",
        "\n",
        "# Creating the model and training\n",
        "logger = Logger(frequency=10)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, ub, lb)\n",
        "def error():\n",
        "  l1, l2 = pinn.get_params(numpy=True)\n",
        "  l1_star, l2_star = lambdas_star\n",
        "  error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
        "  error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
        "  return (error_lambda_1 + error_lambda_2) / 2\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_pred, f_pred = pinn.predict(X_star)\n",
        "lambda_1_pred, lambda_2_pred = pinn.get_params(numpy=True)\n",
        "\n",
        "# Noise case\n",
        "x, t, X, T, Exact_u, X_star, u_star, \\\n",
        "  X_u_train, u_train, ub, lb = prep_data(path, N_u, noise=0.01)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, ub, lb)\n",
        "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
        "lambda_1_pred_noise, lambda_2_pred_noise = pinn.get_params(numpy=True)\n",
        "\n",
        "print(\"l1: \", lambda_1_pred)\n",
        "print(\"l2: \", lambda_2_pred)\n",
        "print(\"l1_noise: \", lambda_1_pred_noise)\n",
        "print(\"l2_noise: \", lambda_2_pred_noise)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.9.1\n",
            "Eager execution: True\n",
            "GPU-accerelated: True\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_3 (Lambda)           (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 20)                60        \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,021\n",
            "Trainable params: 3,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "tf_epoch =      0  elapsed = 00:00  loss = 5.9414e-01  error = 6.1009e-01  l1 = 0.000966  l2 = 0.002479\n",
            "tf_epoch =     10  elapsed = 00:00  loss = 3.0888e-01  error = 6.0845e-01  l1 = 0.003535  l2 = 0.002481\n",
            "tf_epoch =     20  elapsed = 00:01  loss = 2.5273e-01  error = 6.1006e-01  l1 = -0.005578  l2 = 0.002500\n",
            "tf_epoch =     30  elapsed = 00:01  loss = 2.3607e-01  error = 6.1136e-01  l1 = -0.017497  l2 = 0.002530\n",
            "tf_epoch =     40  elapsed = 00:02  loss = 2.1998e-01  error = 6.1247e-01  l1 = -0.030338  l2 = 0.002564\n",
            "tf_epoch =     50  elapsed = 00:03  loss = 2.0089e-01  error = 6.1307e-01  l1 = -0.041687  l2 = 0.002596\n",
            "tf_epoch =     60  elapsed = 00:03  loss = 1.7533e-01  error = 6.1299e-01  l1 = -0.049533  l2 = 0.002621\n",
            "tf_epoch =     70  elapsed = 00:04  loss = 1.4042e-01  error = 6.1422e-01  l1 = -0.051590  l2 = 0.002620\n",
            "tf_epoch =     80  elapsed = 00:04  loss = 1.0898e-01  error = 6.1704e-01  l1 = -0.044107  l2 = 0.002578\n",
            "tf_epoch =     90  elapsed = 00:05  loss = 8.8337e-02  error = 6.1689e-01  l1 = -0.027372  l2 = 0.002526\n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 00:06  loss = 3.7107e-02  error = 5.4643e-01  l1 = 0.134973  l2 = 0.002458\n",
            "nt_epoch =     20  elapsed = 00:07  loss = 3.0625e-02  error = 5.6315e-01  l1 = 0.106129  l2 = 0.002443\n",
            "nt_epoch =     30  elapsed = 00:08  loss = 2.7359e-02  error = 5.6192e-01  l1 = 0.124392  l2 = 0.002393\n",
            "nt_epoch =     40  elapsed = 00:09  loss = 2.5418e-02  error = 5.5352e-01  l1 = 0.151137  l2 = 0.002361\n",
            "nt_epoch =     50  elapsed = 00:10  loss = 2.2819e-02  error = 5.2858e-01  l1 = 0.200031  l2 = 0.002364\n",
            "nt_epoch =     60  elapsed = 00:11  loss = 2.1303e-02  error = 5.1420e-01  l1 = 0.218226  l2 = 0.002398\n",
            "nt_epoch =     70  elapsed = 00:12  loss = 2.0420e-02  error = 4.9349e-01  l1 = 0.229770  l2 = 0.002493\n",
            "nt_epoch =     80  elapsed = 00:13  loss = 1.7810e-02  error = 4.1602e-01  l1 = 0.317170  l2 = 0.002708\n",
            "nt_epoch =     90  elapsed = 00:14  loss = 1.6264e-02  error = 3.8900e-01  l1 = 0.368820  l2 = 0.002716\n",
            "nt_epoch =    100  elapsed = 00:15  loss = 1.4782e-02  error = 3.6332e-01  l1 = 0.394791  l2 = 0.002797\n",
            "nt_epoch =    110  elapsed = 00:16  loss = 1.3688e-02  error = 3.4254e-01  l1 = 0.430496  l2 = 0.002815\n",
            "nt_epoch =    120  elapsed = 00:17  loss = 1.2684e-02  error = 3.3016e-01  l1 = 0.444160  l2 = 0.002850\n",
            "nt_epoch =    130  elapsed = 00:18  loss = 1.2049e-02  error = 3.1195e-01  l1 = 0.483176  l2 = 0.002842\n",
            "nt_epoch =    140  elapsed = 00:19  loss = 1.1594e-02  error = 2.8874e-01  l1 = 0.523040  l2 = 0.002863\n",
            "nt_epoch =    150  elapsed = 00:20  loss = 1.0991e-02  error = 2.6855e-01  l1 = 0.562331  l2 = 0.002867\n",
            "nt_epoch =    160  elapsed = 00:21  loss = 1.0363e-02  error = 2.6749e-01  l1 = 0.564036  l2 = 0.002868\n",
            "nt_epoch =    170  elapsed = 00:22  loss = 9.7247e-03  error = 2.3952e-01  l1 = 0.621490  l2 = 0.002863\n",
            "nt_epoch =    180  elapsed = 00:23  loss = 9.2046e-03  error = 2.2731e-01  l1 = 0.640955  l2 = 0.002879\n",
            "nt_epoch =    190  elapsed = 00:24  loss = 8.3782e-03  error = 2.2741e-01  l1 = 0.649880  l2 = 0.002850\n",
            "nt_epoch =    200  elapsed = 00:25  loss = 8.1608e-03  error = 2.2401e-01  l1 = 0.657853  l2 = 0.002846\n",
            "nt_epoch =    210  elapsed = 00:26  loss = 7.9220e-03  error = 2.1843e-01  l1 = 0.674404  l2 = 0.002829\n",
            "nt_epoch =    220  elapsed = 00:27  loss = 7.7297e-03  error = 2.1978e-01  l1 = 0.679506  l2 = 0.002804\n",
            "nt_epoch =    230  elapsed = 00:28  loss = 7.4354e-03  error = 2.2511e-01  l1 = 0.669400  l2 = 0.002802\n",
            "nt_epoch =    240  elapsed = 00:29  loss = 7.3176e-03  error = 2.2666e-01  l1 = 0.667550  l2 = 0.002798\n",
            "nt_epoch =    250  elapsed = 00:30  loss = 7.0712e-03  error = 2.3336e-01  l1 = 0.660465  l2 = 0.002778\n",
            "nt_epoch =    260  elapsed = 00:31  loss = 6.8256e-03  error = 2.1272e-01  l1 = 0.699659  l2 = 0.002785\n",
            "nt_epoch =    270  elapsed = 00:32  loss = 6.7973e-03  error = 2.1197e-01  l1 = 0.700094  l2 = 0.002788\n",
            "nt_epoch =    280  elapsed = 00:33  loss = 6.6984e-03  error = 2.1386e-01  l1 = 0.696697  l2 = 0.002787\n",
            "nt_epoch =    290  elapsed = 00:34  loss = 6.4396e-03  error = 2.0830e-01  l1 = 0.710921  l2 = 0.002777\n",
            "nt_epoch =    300  elapsed = 00:35  loss = 6.1761e-03  error = 2.1680e-01  l1 = 0.699309  l2 = 0.002760\n",
            "nt_epoch =    310  elapsed = 00:36  loss = 5.9391e-03  error = 2.1554e-01  l1 = 0.704110  l2 = 0.002753\n",
            "nt_epoch =    320  elapsed = 00:37  loss = 5.6878e-03  error = 2.1699e-01  l1 = 0.697976  l2 = 0.002763\n",
            "nt_epoch =    330  elapsed = 00:38  loss = 5.5817e-03  error = 2.1518e-01  l1 = 0.699917  l2 = 0.002768\n",
            "nt_epoch =    340  elapsed = 00:39  loss = 5.4112e-03  error = 2.0904e-01  l1 = 0.712336  l2 = 0.002768\n",
            "nt_epoch =    350  elapsed = 00:40  loss = 5.3169e-03  error = 2.0568e-01  l1 = 0.720434  l2 = 0.002764\n",
            "nt_epoch =    360  elapsed = 00:41  loss = 5.2382e-03  error = 2.0730e-01  l1 = 0.720096  l2 = 0.002754\n",
            "nt_epoch =    370  elapsed = 00:42  loss = 5.1167e-03  error = 2.0089e-01  l1 = 0.729411  l2 = 0.002765\n",
            "nt_epoch =    380  elapsed = 00:43  loss = 5.0194e-03  error = 1.9997e-01  l1 = 0.732116  l2 = 0.002763\n",
            "nt_epoch =    390  elapsed = 00:44  loss = 4.9611e-03  error = 1.9880e-01  l1 = 0.735671  l2 = 0.002759\n",
            "nt_epoch =    400  elapsed = 00:46  loss = 4.8631e-03  error = 1.9917e-01  l1 = 0.735404  l2 = 0.002757\n",
            "nt_epoch =    410  elapsed = 00:47  loss = 4.7974e-03  error = 2.0363e-01  l1 = 0.725041  l2 = 0.002762\n",
            "nt_epoch =    420  elapsed = 00:48  loss = 4.9916e-03  error = 2.0998e-01  l1 = 0.716040  l2 = 0.002750\n",
            "nt_epoch =    430  elapsed = 00:49  loss = 4.6906e-03  error = 2.0225e-01  l1 = 0.728875  l2 = 0.002759\n",
            "nt_epoch =    440  elapsed = 00:50  loss = 4.6556e-03  error = 2.0027e-01  l1 = 0.732612  l2 = 0.002759\n",
            "nt_epoch =    450  elapsed = 00:51  loss = 4.6935e-03  error = 2.0019e-01  l1 = 0.730392  l2 = 0.002767\n",
            "nt_epoch =    460  elapsed = 00:52  loss = 4.6360e-03  error = 2.0079e-01  l1 = 0.731593  l2 = 0.002759\n",
            "nt_epoch =    470  elapsed = 00:53  loss = 5.5616e-03  error = 2.1564e-01  l1 = 0.706035  l2 = 0.002746\n",
            "nt_epoch =    480  elapsed = 00:54  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    490  elapsed = 00:55  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    500  elapsed = 00:56  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    510  elapsed = 00:57  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    520  elapsed = 00:58  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    530  elapsed = 00:59  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    540  elapsed = 01:00  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    550  elapsed = 01:01  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    560  elapsed = 01:02  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    570  elapsed = 01:03  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    580  elapsed = 01:04  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    590  elapsed = 01:05  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    600  elapsed = 01:06  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    610  elapsed = 01:07  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    620  elapsed = 01:08  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    630  elapsed = 01:09  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    640  elapsed = 01:10  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    650  elapsed = 01:11  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    660  elapsed = 01:12  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    670  elapsed = 01:13  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    680  elapsed = 01:14  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    690  elapsed = 01:15  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    700  elapsed = 01:16  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    710  elapsed = 01:17  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    720  elapsed = 01:18  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    730  elapsed = 01:19  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    740  elapsed = 01:20  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    750  elapsed = 01:21  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    760  elapsed = 01:22  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    770  elapsed = 01:23  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    780  elapsed = 01:24  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    790  elapsed = 01:25  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    800  elapsed = 01:26  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    810  elapsed = 01:27  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    820  elapsed = 01:28  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    830  elapsed = 01:29  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    840  elapsed = 01:30  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    850  elapsed = 01:31  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    860  elapsed = 01:32  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    870  elapsed = 01:33  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    880  elapsed = 01:34  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    890  elapsed = 01:35  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    900  elapsed = 01:36  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    910  elapsed = 01:37  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    920  elapsed = 01:38  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    930  elapsed = 01:39  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    940  elapsed = 01:40  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    950  elapsed = 01:41  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    960  elapsed = 01:42  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    970  elapsed = 01:43  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    980  elapsed = 01:44  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "nt_epoch =    990  elapsed = 01:45  loss = nan  error = nan  l1 =   nan  l2 =      nan\n",
            "==================\n",
            "Training finished (epoch 100): duration = 01:46  error = nan  l1 =   nan  l2 =      nan\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_4 (Lambda)           (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 20)                60        \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,021\n",
            "Trainable params: 3,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "tf_epoch =      0  elapsed = 01:47  loss = 4.8225e-01  error = 6.1082e-01  l1 = -0.000975  l2 = 0.002481\n",
            "tf_epoch =     10  elapsed = 01:47  loss = 2.4186e-01  error = 6.0619e-01  l1 = 0.005123  l2 = 0.002491\n",
            "tf_epoch =     20  elapsed = 01:48  loss = 1.7470e-01  error = 6.0491e-01  l1 = -0.006591  l2 = 0.002536\n",
            "tf_epoch =     30  elapsed = 01:48  loss = 1.3521e-01  error = 6.0486e-01  l1 = -0.014973  l2 = 0.002563\n",
            "tf_epoch =     40  elapsed = 01:49  loss = 9.7326e-02  error = 6.0490e-01  l1 = 0.000009  l2 = 0.002515\n",
            "tf_epoch =     50  elapsed = 01:49  loss = 7.4159e-02  error = 6.0221e-01  l1 = 0.025553  l2 = 0.002451\n",
            "tf_epoch =     60  elapsed = 01:50  loss = 5.9209e-02  error = 5.9902e-01  l1 = 0.052594  l2 = 0.002385\n",
            "tf_epoch =     70  elapsed = 01:51  loss = 4.9538e-02  error = 5.9555e-01  l1 = 0.077928  l2 = 0.002327\n",
            "tf_epoch =     80  elapsed = 01:51  loss = 4.3286e-02  error = 5.9184e-01  l1 = 0.097625  l2 = 0.002288\n",
            "tf_epoch =     90  elapsed = 01:52  loss = 3.9408e-02  error = 5.8803e-01  l1 = 0.108247  l2 = 0.002278\n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 01:53  loss = 3.3512e-02  error = 5.8949e-01  l1 = 0.095868  l2 = 0.002308\n",
            "nt_epoch =     20  elapsed = 01:54  loss = 3.0550e-02  error = 5.7169e-01  l1 = 0.128496  l2 = 0.002318\n",
            "nt_epoch =     30  elapsed = 01:55  loss = 2.7607e-02  error = 5.4798e-01  l1 = 0.161292  l2 = 0.002364\n",
            "nt_epoch =     40  elapsed = 01:56  loss = 2.4387e-02  error = 4.9964e-01  l1 = 0.216627  l2 = 0.002496\n",
            "nt_epoch =     50  elapsed = 01:57  loss = 2.1888e-02  error = 4.4475e-01  l1 = 0.265988  l2 = 0.002688\n",
            "nt_epoch =     60  elapsed = 01:58  loss = 1.9215e-02  error = 3.8182e-01  l1 = 0.334071  l2 = 0.002872\n",
            "nt_epoch =     70  elapsed = 01:59  loss = 1.7433e-02  error = 3.4185e-01  l1 = 0.386508  l2 = 0.002960\n",
            "nt_epoch =     80  elapsed = 02:00  loss = 1.6531e-02  error = 3.2637e-01  l1 = 0.399811  l2 = 0.003016\n",
            "nt_epoch =     90  elapsed = 02:01  loss = 1.5758e-02  error = 2.9653e-01  l1 = 0.409329  l2 = 0.003191\n",
            "nt_epoch =    100  elapsed = 02:02  loss = 1.5060e-02  error = 3.1627e-01  l1 = 0.417643  l2 = 0.003343\n",
            "nt_epoch =    110  elapsed = 02:03  loss = 1.4482e-02  error = 3.5215e-01  l1 = 0.463244  l2 = 0.003716\n",
            "nt_epoch =    120  elapsed = 02:04  loss = 1.3661e-02  error = 3.8092e-01  l1 = 0.473604  l2 = 0.003933\n",
            "nt_epoch =    130  elapsed = 02:05  loss = 1.2126e-02  error = 4.2477e-01  l1 = 0.539339  l2 = 0.004421\n",
            "nt_epoch =    140  elapsed = 02:06  loss = 1.0447e-02  error = 4.4980e-01  l1 = 0.590411  l2 = 0.004743\n",
            "nt_epoch =    150  elapsed = 02:07  loss = 9.5420e-03  error = 4.6002e-01  l1 = 0.618026  l2 = 0.004896\n",
            "nt_epoch =    160  elapsed = 02:08  loss = 8.5871e-03  error = 4.8229e-01  l1 = 0.645170  l2 = 0.005124\n",
            "nt_epoch =    170  elapsed = 02:09  loss = 7.8565e-03  error = 5.0008e-01  l1 = 0.688364  l2 = 0.005375\n",
            "nt_epoch =    180  elapsed = 02:10  loss = 7.3140e-03  error = 5.0739e-01  l1 = 0.692883  l2 = 0.005436\n",
            "nt_epoch =    190  elapsed = 02:11  loss = 6.7598e-03  error = 5.1291e-01  l1 = 0.726180  l2 = 0.005577\n",
            "nt_epoch =    200  elapsed = 02:12  loss = 5.8904e-03  error = 5.6808e-01  l1 = 0.776769  l2 = 0.006089\n",
            "nt_epoch =    210  elapsed = 02:13  loss = 5.1299e-03  error = 6.2399e-01  l1 = 0.814783  l2 = 0.006566\n",
            "nt_epoch =    220  elapsed = 02:14  loss = 4.6779e-03  error = 6.1128e-01  l1 = 0.813469  l2 = 0.006481\n",
            "nt_epoch =    230  elapsed = 02:15  loss = 4.1809e-03  error = 6.1906e-01  l1 = 0.841119  l2 = 0.006618\n",
            "nt_epoch =    240  elapsed = 02:16  loss = 3.7075e-03  error = 6.0970e-01  l1 = 0.867545  l2 = 0.006643\n",
            "nt_epoch =    250  elapsed = 02:17  loss = 3.3282e-03  error = 5.9124e-01  l1 = 0.868144  l2 = 0.006527\n",
            "nt_epoch =    260  elapsed = 02:18  loss = 2.9457e-03  error = 5.8311e-01  l1 = 0.886049  l2 = 0.006533\n",
            "nt_epoch =    270  elapsed = 02:19  loss = 2.7420e-03  error = 5.7385e-01  l1 = 0.895165  l2 = 0.006503\n",
            "nt_epoch =    280  elapsed = 02:20  loss = 2.5676e-03  error = 5.5330e-01  l1 = 0.922566  l2 = 0.006459\n",
            "nt_epoch =    290  elapsed = 02:21  loss = 2.3949e-03  error = 5.2946e-01  l1 = 0.935089  l2 = 0.006347\n",
            "nt_epoch =    300  elapsed = 02:22  loss = 2.1077e-03  error = 4.9387e-01  l1 = 0.936374  l2 = 0.006125\n",
            "nt_epoch =    310  elapsed = 02:23  loss = 2.0051e-03  error = 4.8544e-01  l1 = 0.945148  l2 = 0.006099\n",
            "nt_epoch =    320  elapsed = 02:24  loss = 1.8425e-03  error = 4.6155e-01  l1 = 0.939553  l2 = 0.005929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-y6mkKpP5M8",
        "lines_to_next_cell": 0
      },
      "source": [
        "#plot_ide_cont_results(X_star, u_pred, X_u_train, u_train,\n",
        "#  Exact_u, X, T, x, t, lambda_1_pred, lambda_1_pred_noise, lambda_2_pred, lambda_2_pred_noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcY8EfT1w1ev"
      },
      "source": [
        "# 4. Discrete Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFBJUhYqw1ew"
      },
      "source": [
        "$$u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx} = 0$$\n",
        "\n",
        "With $\\lambda_1$ and $\\lambda_2$ real parameters of the differential operator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKxgAhrTw1ex"
      },
      "source": [
        "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
        "$$f := u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx}.$$\n",
        "\n",
        "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
        "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_u}|f(t_u^i,x_u^i)|^2,$$\n",
        "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ respectively the trainring data on $u(t,x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGrMDRc3w1ex"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOuJ6DV1w1ey",
        "lines_to_next_cell": 2
      },
      "source": [
        "\n",
        "# Data size on initial condition on u\n",
        "N_0 = 199\n",
        "N_1 = 201\n",
        "# DeepNN topology (1-sized input [x], 3 hidden layer of 50-width, q-sized output defined later [u_1^n(x), ..., u_{q+1}^n(x)]\n",
        "layers = [1, 50, 50, 50, 0]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 100\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  lr=0.001,\n",
        "  beta_1=0.9,\n",
        "  beta_2=0.999,\n",
        "  epsilon=1e-08)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 2000\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 0.8\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 50\n",
        "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRGW4IW0w1e0"
      },
      "source": [
        "## PINN class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYMIf2_Uw1e0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yiB3TOYw1e1"
      },
      "source": [
        "\n",
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta):\n",
        "    self.lb = lb\n",
        "    self.ub = ub\n",
        "\n",
        "    self.dt = dt\n",
        "\n",
        "    self.q = max(q,1)\n",
        "    self.IRK_alpha = IRK_alpha\n",
        "    self.IRK_beta = IRK_beta\n",
        "\n",
        "    # Descriptive Keras model [2, 50, …, 50, q+1]\n",
        "    self.U_model = tf.keras.Sequential()\n",
        "    self.U_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.U_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:]:\n",
        "        self.U_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "  def __autograd(self, U, x, dummy):\n",
        "    # Using the new GradientTape paradigm of TF2.0,\n",
        "    # which keeps track of operations to get the gradient at runtime\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Watching the two inputs we’ll need later, x and t\n",
        "      tape.watch(x)\n",
        "      tape.watch(dummy)\n",
        "\n",
        "      # Getting the prediction\n",
        "      U = self.U_model(x) # shape=(len(x), q)\n",
        "\n",
        "      # Deriving INSIDE the tape (2-step-dummy grad technique because U is a mat)\n",
        "      g_U = tape.gradient(U, x, output_gradients=dummy)\n",
        "      U_x = tape.gradient(g_U, dummy)\n",
        "      g_U_x = tape.gradient(U_x, x, output_gradients=dummy)\n",
        "    \n",
        "    # Doing the last one outside the with, to optimize performance\n",
        "    # Impossible to do for the earlier grad, because they’re needed after\n",
        "    U_xx = tape.gradient(g_U_x, dummy)\n",
        "\n",
        "    # Letting the tape go\n",
        "    del tape\n",
        "    return U_x, U_xx\n",
        "\n",
        "  def U_0_model(self, x, customDummy=None):\n",
        "    U = self.U_model(x)\n",
        "    if customDummy != None:\n",
        "      dummy = customDummy\n",
        "    else:\n",
        "      dummy = self.dummy_x_0\n",
        "    U_x, U_xx = self.__autograd(U, x, dummy)\n",
        "\n",
        "    # Buidling the PINNs\n",
        "    l1 = self.lambda_1\n",
        "    l2 = tf.exp(self.lambda_2)\n",
        "    N = l1*U*U_x - l2*U_xx # shape=(len(x), q)\n",
        "    return U + self.dt*tf.matmul(N, self.IRK_alpha.T)\n",
        "\n",
        "  def U_1_model(self, x, customDummy=None):\n",
        "    U = self.U_model(x)\n",
        "    #dummy = customDummy or self.dummy_x_1\n",
        "    if customDummy != None:\n",
        "      dummy = customDummy\n",
        "    else:\n",
        "      dummy = self.dummy_x_1\n",
        "    U_x, U_xx = self.__autograd(U, x, dummy)\n",
        "\n",
        "    # Buidling the PINNs, shape = (len(x), q+1), IRK shape = (q, q+1)\n",
        "    l1 = self.lambda_1\n",
        "    l2 = tf.exp(self.lambda_2)\n",
        "    N = -l1*U*U_x + l2*U_xx # shape=(len(x), q)\n",
        "    return U + self.dt*tf.matmul(N, (self.IRK_beta - self.IRK_alpha).T)\n",
        "\n",
        "  # Defining custom loss\n",
        "  def __loss(self, x_0, u_0, x_1, u_1):\n",
        "    u_0_pred = self.U_0_model(x_0)\n",
        "    u_1_pred = self.U_1_model(x_1)\n",
        "    return tf.reduce_sum(tf.square(u_0_pred - u_0)) + \\\n",
        "      tf.reduce_sum(tf.square(u_1_pred - u_1))\n",
        "\n",
        "  def __grad(self, x_0, u_0, x_1, u_1):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(x_0, u_0, x_1, u_1)\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.U_model.trainable_variables\n",
        "    var.extend([self.lambda_1, self.lambda_2])\n",
        "    return var\n",
        "\n",
        "  def get_weights(self):\n",
        "      w = []\n",
        "      for layer in self.U_model.layers[1:]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "      w.extend(self.lambda_1.numpy())\n",
        "      w.extend(self.lambda_2.numpy())\n",
        "      return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.U_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "    self.lambda_1.assign([w[-2]])\n",
        "    self.lambda_2.assign([w[-1]])\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    l1 = self.lambda_1\n",
        "    l2 = tf.exp(self.lambda_2)\n",
        "    if numpy:\n",
        "      return l1.numpy()[0], l2.numpy()[0]\n",
        "    return l1, l2\n",
        "\n",
        "  def summary(self):\n",
        "    return self.U_model.summary()\n",
        "\n",
        "  def __createDummy(self, x):\n",
        "    return tf.ones([x.shape[0], self.q], dtype=self.dtype)\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, x_0, u_0, x_1, u_1, tf_epochs=1):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    x_0 = tf.convert_to_tensor(x_0, dtype=self.dtype)\n",
        "    u_0 = tf.convert_to_tensor(u_0, dtype=self.dtype)\n",
        "    x_1 = tf.convert_to_tensor(x_1, dtype=self.dtype)\n",
        "    u_1 = tf.convert_to_tensor(u_1, dtype=self.dtype)\n",
        "\n",
        "    self.lambda_1 = tf.Variable([0.0], dtype=self.dtype)\n",
        "    self.lambda_2 = tf.Variable([-6.0], dtype=self.dtype)\n",
        "\n",
        "    # Creating dummy tensors for the gradients\n",
        "    self.dummy_x_0 = self.__createDummy(x_0)\n",
        "    self.dummy_x_1 = self.__createDummy(x_1)\n",
        "\n",
        "    def log_train_epoch(epoch, loss, is_iter):\n",
        "      l1, l2 = self.get_params(numpy=True)\n",
        "      custom = f\"l1 = {l1:5f}  l2 = {l2:8f}\"\n",
        "      self.logger.log_train_epoch(epoch, loss, custom, is_iter)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(x_0, u_0, x_1, u_1)\n",
        "      self.optimizer.apply_gradients(\n",
        "        zip(grads, self.__wrap_training_variables()))\n",
        "      log_train_epoch(epoch, loss_value, False)\n",
        "    \n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        tape.watch(self.lambda_1)\n",
        "        tape.watch(self.lambda_2)\n",
        "        loss_value = self.__loss(x_0, u_0, x_1, u_1)\n",
        "      grad = tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "    # tfp.optimizer.lbfgs_minimize(\n",
        "    #   loss_and_flat_grad,\n",
        "    #   initial_position=self.get_weights(),\n",
        "    #   num_correction_pairs=nt_config.nCorrection,\n",
        "    #   max_iterations=nt_config.maxIter,\n",
        "    #   f_relative_tolerance=nt_config.tolFun,\n",
        "    #   tolerance=nt_config.tolFun,\n",
        "    #   parallel_iterations=6)\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights(),\n",
        "      nt_config, Struct(), True, log_train_epoch)\n",
        "    \n",
        "    l1, l2 = self.get_params(numpy=True)\n",
        "    self.logger.log_train_end(tf_epochs, f\"l1 = {l1:5f}  l2 = {l2:8f}\")\n",
        "\n",
        "  def predict(self, x_star):\n",
        "    x_star = tf.convert_to_tensor(x_star, dtype=self.dtype)\n",
        "    dummy = self.__createDummy(x_star)\n",
        "    U_0_star = self.U_0_model(x_star, dummy)\n",
        "    U_1_star = self.U_1_model(x_star, dummy)\n",
        "    return U_0_star, U_1_star"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFS_u67iw1e2"
      },
      "source": [
        "## Training and plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU5VfXKsw1e3",
        "lines_to_next_cell": 2
      },
      "source": [
        "\n",
        "# Setup\n",
        "lb = np.array([-1.0])\n",
        "ub = np.array([1.0])\n",
        "idx_t_0 = 10\n",
        "skip = 80\n",
        "idx_t_1 = idx_t_0 + skip\n",
        "\n",
        "# Getting the data\n",
        "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
        "x_0, u_0, x_1, u_1, x_star, t_star, dt, q, \\\n",
        "  Exact_u, IRK_alpha, IRK_beta = prep_data(path, N_0=N_0, N_1=N_1,\n",
        "  lb=lb, ub=ub, noise=0.0, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
        "lambdas_star = (1.0, 0.01/np.pi)\n",
        "\n",
        "# Setting the output layer dynamically\n",
        "layers[-1] = q\n",
        " \n",
        "# Creating the model and training\n",
        "logger = Logger(frequency=10)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta)\n",
        "def error():\n",
        "  l1, l2 = pinn.get_params(numpy=True)\n",
        "  l1_star, l2_star = lambdas_star\n",
        "  error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
        "  error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
        "  return (error_lambda_1 + error_lambda_2) / 2\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(x_0, u_0, x_1, u_1, tf_epochs)\n",
        "\n",
        "# Getting the model predictions\n",
        "U_0_pred, U_1_pred = pinn.predict(x_star)\n",
        "lambda_1_pred, lambda_2_pred = pinn.get_params(numpy=True)\n",
        "\n",
        "# Noisy case (same as before with a different noise)\n",
        "x_0, u_0, x_1, u_1, x_star, t_star, dt, q, \\\n",
        "  Exact_u, IRK_alpha, IRK_beta = prep_data(path, N_0=N_0, N_1=N_1,\n",
        "  lb=lb, ub=ub, noise=0.01, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
        "layers[-1] = q\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta)\n",
        "pinn.fit(x_0, u_0, x_1, u_1, tf_epochs)\n",
        "U_0_pred, U_1_pred = pinn.predict(x_star)\n",
        "lambda_1_pred_noisy, lambda_2_pred_noisy = pinn.get_params(numpy=True)\n",
        "\n",
        "print(\"l1: \", lambda_1_pred)\n",
        "print(\"l2: \", lambda_2_pred)\n",
        "print(\"noisy l1: \", lambda_1_pred_noisy)\n",
        "print(\"noisy l2: \", lambda_2_pred_noisy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUtmuGlpw1e6"
      },
      "source": [
        "#plot_ide_disc_results(x_star, t_star, idx_t_0, idx_t_1, x_0, u_0, x_1, u_1,\n",
        "#  ub, lb, U_1_pred, Exact_u, lambda_1_pred, lambda_1_pred_noisy, lambda_2_pred, lambda_2_pred_noisy, x_star, t_star)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}